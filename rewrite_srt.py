import openai
import time
import os
import json
import uuid
from dotenv import load_dotenv
from tqdm import tqdm

load_dotenv()

# Initialize the OpenAI client
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# --- Configuration ---
MODEL_PRICING = {
    # Prices per 1K tokens
    "gpt-4o-mini": {"prompt": 0.00000015, "completion": 0.0000006},
    "gpt-3.5-turbo": {"prompt": 0.0000005, "completion": 0.0000015},
    "gpt-4o": {"prompt": 0.000005, "completion": 0.000015},
    # Add other models and their pricing here if needed
}
rewrite_scripts_DIRECTORY = "rewrite_scripts" # Directory to store rewritten scripts
REWRITE_SUMMARIES_DIRECTORY = "rewrite_summaries" # Directory to store summaries for rewriting
DEFAULT_BATCH_SIZE = 10 # Default batch size for rewriting
DEFAULT_SUMMARIZATION_MODEL = "gpt-4o-mini" # Default model for summarization
DEFAULT_TITLE_DESCRIPTION_MODEL = "gpt-4o-mini" # Default model for titles & descriptions in rewrite context
NARRATIVE_SEGMENT_DURATION_SECONDS = 6 # Estimated duration for each auto-generated narrative segment
SUMMARY_SECTION_MARKERS = {
    "summary": "[CONTEXTUAL SUMMARY]",
    "titles": "[SUGGESTED TITLES]",
    "description": "[SUGGESTED DESCRIPTION]"
}

# --- Helper Functions ---

def srt_time_to_seconds(time_str: str) -> float:
    """Converts SRT time format (HH:MM:SS,ms) to seconds."""
    parts = time_str.split(',')
    hms = parts[0].split(':')
    return int(hms[0]) * 3600 + int(hms[1]) * 60 + int(hms[2]) + int(parts[1]) / 1000

def get_srt_total_duration(srt_blocks: list[str]) -> float:
    """Calculates the total duration of an SRT file from its blocks."""
    if not srt_blocks:
        return 0.0
    try:
        last_block = srt_blocks[-1]
        lines = last_block.split('\n')
        if len(lines) >= 2:
            timecode_line = lines[1]
            _, end_time_str = timecode_line.split(' --> ')
            return srt_time_to_seconds(end_time_str)
    except Exception as e:
        print(f"L·ªói khi t√≠nh to√°n t·ªïng th·ªùi l∆∞·ª£ng SRT: {e}. Tr·∫£ v·ªÅ 0.")
    return 0.0

def load_srt_blocks(input_path: str) -> list[str]:
    """Reads an SRT file and splits it into content blocks."""
    print(f"ƒêang ƒë·ªçc file: {input_path}")
    try:
        with open(input_path, "r", encoding="utf-8") as f:
            return f.read().strip().split("\n\n")
    except FileNotFoundError:
        print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file {input_path}")
        return []
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file {input_path}: {e}")
        return []

def generate_contextual_summary(srt_blocks: list[str], model: str) -> tuple[str, int, int]:
    """Generates a contextual summary of the entire SRT content to aid rewriting."""
    print(f"\nƒêang t·∫°o t√≥m t·∫Øt ng·ªØ c·∫£nh cho vi·ªác vi·∫øt l·∫°i v·ªõi model: {model}...")
    full_text_for_summary = []
    for block_text in srt_blocks:
        lines = block_text.split("\n")
        if len(lines) >= 3:
            full_text_for_summary.append(" ".join(lines[2:]))
    
    if not full_text_for_summary:
        print("Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ t·∫°o t√≥m t·∫Øt.")
        return "", 0, 0

    combined_text = "\n---".join(full_text_for_summary) # Removed extra newline

    summary_prompt = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√¢n t√≠ch n·ªôi dung chuy√™n s√¢u. D·ª±a v√†o to√†n b·ªô n·ªôi dung k·ªãch b·∫£n g·ªëc ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y, "
        "h√£y vi·∫øt m·ªôt b·∫£n t√≥m t·∫Øt b·∫±ng ti·∫øng Vi·ªát (kho·∫£ng 200-350 t·ª´). "
        "B·∫£n t√≥m t·∫Øt n√†y c√≥ hai m·ª•c ƒë√≠ch ch√≠nh: (1) Cung c·∫•p th√¥ng tin n·ªÅn v·ªÅ b·ªëi c·∫£nh, c·ªët truy·ªán t·ªïng th·ªÉ, c√°c nh√¢n v·∫≠t quan tr·ªçng, m·ªëi quan h·ªá v√† ƒë·ªông c∆° c·ªßa h·ªç, c≈©ng nh∆∞ gi·ªçng ƒëi·ªáu v√† kh√¥ng kh√≠ chung c·ªßa k·ªãch b·∫£n. (2) QUAN TR·ªåNG H∆†N: X√°c ƒë·ªãnh v√† li·ªát k√™ m·ªôt c√°ch r√µ r√†ng C√ÅC √ù CH√çNH HO·∫∂C C√ÅC S·ª∞ KI·ªÜN N√öT TH·∫ÆT MANG T√çNH QUY·∫æT ƒê·ªäNH c·ªßa c√¢u chuy·ªán, t·ªët nh·∫•t l√† theo tr√¨nh t·ª± th·ªùi gian di·ªÖn ra. "
        "Nh·ªØng √Ω ch√≠nh/s·ª± ki·ªán n√∫t th·∫Øt n√†y l√† nh·ªØng ƒëi·ªÉm c·ªët l√µi m√† m·ªôt phi√™n b·∫£n k·ªÉ l·∫°i t·ª± s·ª± SAU N√ÄY PH·∫¢I ƒê·ªÄ C·∫¨P ƒê·∫æN ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ƒë·∫ßy ƒë·ªß v√† trung th·ª±c v·ªõi tinh th·∫ßn c·ªßa t√°c ph·∫©m g·ªëc. "
        "Ph·∫ßn li·ªát k√™ c√°c √Ω ch√≠nh n√†y n√™n ƒë∆∞·ª£c tr√¨nh b√†y m·ªôt c√°ch m·∫°ch l·∫°c, d·ªÖ hi·ªÉu, c√≥ th·ªÉ ·ªü d·∫°ng g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c m·ªôt ƒëo·∫°n vƒÉn ri√™ng bi·ªát n√™u b·∫≠t c√°c ƒëi·ªÉm n√†y m·ªôt c√°ch c√¥ ƒë·ªçng. "
        "V√≠ d·ª• c√°ch tr√¨nh b√†y c√°c √Ω ch√≠nh (n·∫øu d√πng g·∫°ch ƒë·∫ßu d√≤ng): \n"
        "- S·ª± ki·ªán A m·ªü ƒë·∫ßu c√¢u chuy·ªán, gi·ªõi thi·ªáu nh√¢n v·∫≠t X.\n"
        "- Xung ƒë·ªôt ch√≠nh n·∫£y sinh khi Y xu·∫•t hi·ªán.\n"
        "- Nh√¢n v·∫≠t X ƒë∆∞a ra quy·∫øt ƒë·ªãnh quan tr·ªçng Z.\n"
        "- Cao tr√†o c·ªßa c√¢u chuy·ªán l√† s·ª± ki·ªán K.\n"
        "- C√¢u chuy·ªán k·∫øt th√∫c v·ªõi h·∫≠u qu·∫£ M v√† b√†i h·ªçc N.\n"
        "M·ª•c ti√™u cu·ªëi c√πng l√† t·∫°o ra m·ªôt b·∫£n t√≥m t·∫Øt kh√¥ng ch·ªâ m√¥ t·∫£ m√† c√≤n cung c·∫•p m·ªôt 'd√†n √Ω c·ªët truy·ªán' v·ªØng ch·∫Øc, ch·ª©a ƒë·ª±ng nh·ªØng y·∫øu t·ªë kh√¥ng th·ªÉ thi·∫øu, ƒë·ªÉ h·ªó tr·ª£ vi·ªác t√°i t·∫°o t·ª± s·ª± m·ªôt c√°ch s√°ng t·∫°o nh∆∞ng v·∫´n b√°m s√°t c√°c di·ªÖn bi·∫øn v√† th√¥ng ƒëi·ªáp quan tr·ªçng nh·∫•t c·ªßa k·ªãch b·∫£n g·ªëc.\n\nN·ªòI DUNG K·ªäCH B·∫¢N G·ªêC:\n"
        f"{combined_text}"
    )

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω ph√¢n t√≠ch n·ªôi dung chuy√™n nghi·ªáp, c√≥ kh·∫£ nƒÉng n·∫Øm b·∫Øt c√°c y·∫øu t·ªë quan tr·ªçng c·ªßa k·ªãch b·∫£n ƒë·ªÉ h·ªó tr·ª£ vi·ªác vi·∫øt l·∫°i. Ch·ªâ cung c·∫•p t√≥m t·∫Øt b·∫±ng ti·∫øng Vi·ªát."},
                {"role": "user", "content": summary_prompt}
            ],
            temperature=0.5,
            max_tokens=600
        )
        summary = response.choices[0].message.content
        prompt_tokens = response.usage.prompt_tokens if response.usage else 0
        completion_tokens = response.usage.completion_tokens if response.usage else 0
        print("ƒê√£ t·∫°o xong t√≥m t·∫Øt ng·ªØ c·∫£nh cho vi·ªác vi·∫øt l·∫°i.")
        return summary.strip(), prompt_tokens, completion_tokens
    except Exception as e:
        print(f"L·ªói khi t·∫°o t√≥m t·∫Øt ng·ªØ c·∫£nh cho vi·ªác vi·∫øt l·∫°i: {e}")
        return "", 0, 0

def parse_structured_summary_content(file_content: str) -> tuple[str, list[str], str]:
    """Parses the structured content from a summary file."""
    summary_text = ""
    titles_list = []
    description_text = ""
    current_section = None
    for line in file_content.splitlines():
        if line.strip() == SUMMARY_SECTION_MARKERS["summary"]:
            current_section = "summary"
            continue
        elif line.strip() == SUMMARY_SECTION_MARKERS["titles"]:
            current_section = "titles"
            continue
        elif line.strip() == SUMMARY_SECTION_MARKERS["description"]:
            current_section = "description"
            continue
        if current_section == "summary":
            summary_text += line + "\n"
        elif current_section == "titles":
            if line.strip():
                cleaned_line = line.strip()
                if len(cleaned_line) > 2 and cleaned_line[0].isdigit() and cleaned_line[1] == '.':
                    titles_list.append(cleaned_line[2:].strip())
                elif len(cleaned_line) > 1 and cleaned_line[0] == '-':
                    titles_list.append(cleaned_line[1:].strip())
                else:
                    titles_list.append(cleaned_line)
        elif current_section == "description":
            description_text += line + "\n"
    return summary_text.strip(), titles_list, description_text.strip()

def get_or_create_summary(
    srt_blocks: list[str], 
    summary_model: str, 
    title_model: str, 
    description_model: str, 
    summary_file_path: str
) -> tuple[str, list[str], str, int, int]:
    """Loads or generates summary, titles, and description for rewriting context."""
    total_prompt_tokens = 0
    total_completion_tokens = 0
    if os.path.exists(summary_file_path):
        try:
            with open(summary_file_path, "r", encoding="utf-8") as f:
                file_content = f.read()
            summary_text, titles_list, description_text = parse_structured_summary_content(file_content)
            if summary_text and titles_list and description_text:
                print(f"ƒê√£ t·∫£i t√≥m t·∫Øt, ti√™u ƒë·ªÅ, m√¥ t·∫£ (vi·∫øt l·∫°i) t·ª´: {summary_file_path}")
                return summary_text, titles_list, description_text, 0, 0
            else:
                print(f"File t√≥m t·∫Øt (vi·∫øt l·∫°i) {summary_file_path} kh√¥ng ƒë·ªß/l·ªói. S·∫Ω t·∫°o m·ªõi.")
        except Exception as e:
            print(f"L·ªói ƒë·ªçc/ph√¢n t√≠ch file t√≥m t·∫Øt (vi·∫øt l·∫°i) {summary_file_path}: {e}. S·∫Ω t·∫°o m·ªõi.")

    summary_text, s_p, s_c = generate_contextual_summary(srt_blocks, summary_model)
    total_prompt_tokens += s_p
    total_completion_tokens += s_c
    titles_list = []
    description_text = ""
    if summary_text:
        titles_list, t_p, t_c = generate_creative_titles_for_rewrite(summary_text, title_model)
        total_prompt_tokens += t_p
        total_completion_tokens += t_c
        description_text, d_p, d_c = generate_engaging_description_for_rewrite(summary_text, description_model)
        total_prompt_tokens += d_p
        total_completion_tokens += d_c
    else:
        print("Kh√¥ng t·∫°o ƒë∆∞·ª£c t√≥m t·∫Øt (vi·∫øt l·∫°i), b·ªè qua t·∫°o ti√™u ƒë·ªÅ/m√¥ t·∫£.")
    if summary_text or titles_list or description_text:
        try:
            os.makedirs(os.path.dirname(summary_file_path), exist_ok=True)
            with open(summary_file_path, "w", encoding="utf-8") as f:
                f.write(f"{SUMMARY_SECTION_MARKERS['summary']}\n{summary_text}\n\n")
                f.write(f"{SUMMARY_SECTION_MARKERS['titles']}\n")
                for i, title in enumerate(titles_list):
                    f.write(f"{i+1}. {title}\n")
                f.write("\n")
                f.write(f"{SUMMARY_SECTION_MARKERS['description']}\n{description_text}\n")
            print(f"ƒê√£ l∆∞u t√≥m t·∫Øt, ti√™u ƒë·ªÅ, m√¥ t·∫£ (vi·∫øt l·∫°i) v√†o: {summary_file_path}")
        except Exception as e:
            print(f"L·ªói l∆∞u file t√≥m t·∫Øt t·ªïng h·ª£p (vi·∫øt l·∫°i) {summary_file_path}: {e}")
    return summary_text, titles_list, description_text, total_prompt_tokens, total_completion_tokens

def generate_creative_titles_for_rewrite(context_summary: str, model: str) -> tuple[list[str], int, int]:
    """Generates creative titles for rewriting context, optimized for TikTok SEO."""
    print(f"\nƒêang t·∫°o ti√™u ƒë·ªÅ s√°ng t·∫°o (TikTok SEO, vi·∫øt l·∫°i) v·ªõi model: {model}...")
    if not context_summary: return [], 0, 0
    prompt = (
        "B·∫°n l√† m·ªôt chuy√™n gia s√°ng t·∫°o n·ªôi dung v√† t·ªëi ∆∞u SEO cho TikTok. "
        "D·ª±a v√†o t√≥m t·∫Øt k·ªãch b·∫£n ƒë∆∞·ª£c cung c·∫•p, h√£y ƒë·ªÅ xu·∫•t 3-5 ti√™u ƒë·ªÅ video ti·∫øng Vi·ªát c·ª±c k·ª≥ thu h√∫t cho TikTok. "
        "C√°c ti√™u ƒë·ªÅ n√†y c·∫ßn:\n"
        "1. Ng·∫Øn g·ªçn (t·ªëi ƒëa 70-100 k√Ω t·ª± n·∫øu c√≥ th·ªÉ, ho·∫∑c 1-2 d√≤ng ng·∫Øn tr√™n m√†n h√¨nh TikTok).\n"
        "2. T·∫°o„Éï„ÉÉ„ÇØ m·∫°nh (strong hook): G√¢y ·∫•n t∆∞·ª£ng ngay t·ª´ nh·ªØng t·ª´ ƒë·∫ßu ti√™n.\n"
        "3. K√≠ch th√≠ch t√≤ m√≤ cao ƒë·ªô: Khi·∫øn ng∆∞·ªùi xem ph·∫£i d·ª´ng l·∫°i v√† xem video ngay l·∫≠p t·ª©c.\n"
        "4. C√≥ th·ªÉ g·ª£i √Ω t·ª´ kh√≥a ho·∫∑c ch·ªß ƒë·ªÅ ƒëang th·ªãnh h√†nh (n·∫øu c√≥ th·ªÉ suy lu·∫≠n chung t·ª´ n·ªôi dung).\n"
        "5. Ph·∫£n √°nh n·ªôi dung c·ªët l√µi ho·∫∑c ƒëi·ªÉm ƒë·∫∑c s·∫Øc nh·∫•t c·ªßa k·ªãch b·∫£n (ƒë√£ ƒë∆∞·ª£c t√≥m t·∫Øt) theo m·ªôt c√°ch g√¢y s·ªëc ho·∫∑c b·∫•t ng·ªù.\n\n"
        f"T√ìM T·∫ÆT K·ªäCH B·∫¢N: '''{context_summary}'''\n\n"
        "V√≠ d·ª• phong c√°ch (kh√¥ng ph·∫£i n·ªôi dung): 'S·ª± th·∫≠t ƒë·∫±ng sau... [G√¢y S·ªëc]', 'B·∫°n s·∫Ω KH√îNG TIN N·ªîI khi th·∫•y...', 'POV: L·∫ßn ƒë·∫ßu t√¥i...'\n"
        "Ch·ªâ cung c·∫•p 3-5 ti√™u ƒë·ªÅ, m·ªói ti√™u ƒë·ªÅ m·ªôt d√≤ng."
    )
    try:
        response = client.chat.completions.create(
            model=model, messages=[{"role": "system", "content": "Chuy√™n gia s√°ng t·∫°o ti√™u ƒë·ªÅ video TikTok ti·∫øng Vi·ªát, ∆∞u ti√™n s·ª± ng·∫Øn g·ªçn, g√¢y s·ªëc v√† t√≤ m√≤."}, {"role": "user", "content": prompt}],
            temperature=0.8, # Slightly higher for more viral-style creativity
            max_tokens=250 
        )
        titles = [t.strip() for t in response.choices[0].message.content.split("\n") if t.strip()]
        return titles, response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0
    except Exception as e:
        print(f"L·ªói t·∫°o ti√™u ƒë·ªÅ (vi·∫øt l·∫°i, TikTok): {e}"); return [], 0, 0

def generate_engaging_description_for_rewrite(context_summary: str, model: str) -> tuple[str, int, int]:
    """Generates an engaging description for rewriting context, optimized for TikTok SEO."""
    print(f"ƒêang t·∫°o m√¥ t·∫£ h·∫•p d·∫´n (TikTok SEO, vi·∫øt l·∫°i) v·ªõi model: {model}...")
    if not context_summary: return "", 0, 0
    prompt = (
        "B·∫°n l√† m·ªôt chuy√™n gia s√°ng t·∫°o n·ªôi dung v√† copywriter cho TikTok. "
        "D·ª±a v√†o t√≥m t·∫Øt k·ªãch b·∫£n ƒë∆∞·ª£c cung c·∫•p, h√£y vi·∫øt m·ªôt m√¥ t·∫£ (caption) video TikTok b·∫±ng ti·∫øng Vi·ªát si√™u ng·∫Øn (t·ªëi ƒëa 1-3 c√¢u, kho·∫£ng 100-220 k√Ω t·ª±). "
        "M√¥ t·∫£ n√†y c·∫ßn:\n"
        "1. C√≥ M√ìC C√ÇU C·ª∞C M·∫†NH ·ªü ngay ƒë·∫ßu.\n"
        "2. T·∫°o s·ª± t√≤ m√≤ ho·∫∑c ƒë·∫∑t c√¢u h·ªèi ƒë·ªÉ khuy·∫øn kh√≠ch t∆∞∆°ng t√°c (b√¨nh lu·∫≠n, xem h·∫øt video).\n"
        "3. Li√™n quan tr·ª±c ti·∫øp ƒë·∫øn ƒëi·ªÉm h·∫•p d·∫´n nh·∫•t c·ªßa video (d·ª±a tr√™n t√≥m t·∫Øt).\n"
        "4. C√≥ th·ªÉ bao g·ªìm 2-3 hashtag g·ª£i √Ω chung chung li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ (v√≠ d·ª•: #learnontiktok #bian #khampha #kechuyen). Tr√°nh hashtag qu√° c·ª• th·ªÉ ho·∫∑c trend nh·∫•t th·ªùi m√† AI c√≥ th·ªÉ kh√¥ng bi·∫øt.\n\n"
        f"T√ìM T·∫ÆT K·ªäCH B·∫¢N: '''{context_summary}'''\n\n"
        "V√≠ d·ª• phong c√°ch caption (kh√¥ng ph·∫£i n·ªôi dung): 'Kh√¥ng th·ªÉ tin ƒë∆∞·ª£c chuy·ªán g√¨ ƒë√£ x·∫£y ra üò± Xem ngay ƒë·ªÉ bi·∫øt! #shock #batngo', 'B·∫°n nghƒ© sao v·ªÅ ƒëi·ªÅu n√†y? ü§î Comment ngay! #learnontiktok #xuhuong'\n"
        "Ch·ªâ cung c·∫•p n·ªôi dung m√¥ t·∫£ (caption) v√† c√°c hashtag ƒë·ªÅ xu·∫•t."
    )
    try:
        response = client.chat.completions.create(
            model=model, messages=[{"role": "system", "content": "Chuy√™n gia vi·∫øt caption TikTok ti·∫øng Vi·ªát si√™u thu h√∫t, ng·∫Øn g·ªçn, k√®m hashtag li√™n quan."}, {"role": "user", "content": prompt}],
            temperature=0.75, # Maintain creativity for description
            max_tokens=200 # Descriptions are short
        )
        return response.choices[0].message.content.strip(), response.usage.prompt_tokens if response.usage else 0, response.usage.completion_tokens if response.usage else 0
    except Exception as e:
        print(f"L·ªói t·∫°o m√¥ t·∫£ (vi·∫øt l·∫°i, TikTok): {e}"); return "", 0, 0

def generate_new_narrative_segments(context_summary: str, original_srt_full_text: str, target_duration_seconds: float, model: str, narrative_goal_prompt_template: str) -> tuple[list[str], int, int]:
    """Generates new narrative segments based on summary, full original text, target duration, and a goal prompt template."""
    print(f"\nƒêang t·∫°o c√°c ph√¢n ƒëo·∫°n t·ª± s·ª± m·ªõi v·ªõi model: {model}...")
    
    if not context_summary and not original_srt_full_text:
        print("L·ªói: C·∫ßn t√≥m t·∫Øt ng·ªØ c·∫£nh ho·∫∑c to√†n b·ªô k·ªãch b·∫£n g·ªëc ƒë·ªÉ t·∫°o t·ª± s·ª± m·ªõi.")
        return [], 0, 0

    # Construct the dynamic part of the prompt regarding duration
    duration_guidance = f"T·ªïng th·ªùi l∆∞·ª£ng c·ªßa k·ªãch b·∫£n g·ªëc l√† kho·∫£ng {target_duration_seconds:.0f} gi√¢y. K·ªãch b·∫£n t·ª± s·ª± m·ªõi b·∫°n t·∫°o ra n√™n c√≥ t·ªïng th·ªùi l∆∞·ª£ng t∆∞∆°ng ƒë∆∞∆°ng ho·∫∑c d√†i h∆°n m·ªôt ch√∫t. H√£y c√¢n nh·∫Øc ƒëi·ªÅu n√†y khi quy·∫øt ƒë·ªãnh s·ªë l∆∞·ª£ng v√† ƒë·ªô d√†i c·ªßa c√°c ph√¢n ƒëo·∫°n."
    if target_duration_seconds == 0:
        duration_guidance = "Kh√¥ng c√≥ th√¥ng tin th·ªùi l∆∞·ª£ng g·ªëc, h√£y t·∫≠p trung v√†o vi·ªác t·∫°o ƒë·ªß s·ªë l∆∞·ª£ng ph√¢n ƒëo·∫°n ƒë·ªÉ bao ph·ªß n·ªôi dung m·ªôt c√°ch h·ª£p l√Ω."

    full_prompt = (
        f"{narrative_goal_prompt_template.format(duration_guidance=duration_guidance)}\n\n"
        "T√ìM T·∫ÆT K·ªäCH B·∫¢N ƒê·ªÇ THAM KH·∫¢O (cho b·ªëi c·∫£nh v√† gi·ªçng ƒëi·ªáu t·ªïng th·ªÉ):\n"
        f"'''{context_summary}'''\n\n"
        "K·ªäCH B·∫¢N G·ªêC ƒê·∫¶Y ƒê·ª¶ (ƒë·ªÉ ƒë·∫£m b·∫£o b√°m s√°t c√°c s·ª± ki·ªán v√† th√¥ng tin chi ti·∫øt):\n"
        f"'''{original_srt_full_text}'''\n\n"
        "L∆ØU √ù QUAN TR·ªåNG: Ch·ªâ tr·∫£ v·ªÅ c√°c ƒëo·∫°n vƒÉn b·∫£n, m·ªói ƒëo·∫°n c√°ch nhau b·ªüi d·∫•u ph√¢n t√°ch '||NEW_SEGMENT||'. Kh√¥ng th√™m b·∫•t k·ª≥ gi·∫£i th√≠ch, ti√™u ƒë·ªÅ, hay ƒë·ªãnh d·∫°ng n√†o kh√°c."
    )

    prompt_tokens = 0
    completion_tokens = 0
    segments = []

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "B·∫°n l√† m·ªôt nh√† vƒÉn s√°ng t·∫°o, chuy√™n gia x√¢y d·ª±ng k·ªãch b·∫£n t·ª± s·ª± h·∫•p d·∫´n b·∫±ng ti·∫øng Vi·ªát. Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o ra c√°c ƒëo·∫°n vƒÉn b·∫£n t∆∞·ªùng thu·∫≠t d·ª±a tr√™n t√≥m t·∫Øt v√† h∆∞·ªõng d·∫´n ƒë∆∞·ª£c cung c·∫•p. Lu√¥n tr·∫£ l·ªùi b·∫±ng c√°c ƒëo·∫°n vƒÉn b·∫£n ph√¢n t√°ch b·ªüi '||NEW_SEGMENT||'."},
                {"role": "user", "content": full_prompt}
            ],
            temperature=0.75, # Slightly higher for more creativity
            max_tokens=3000  # Allow for a decent length narrative
        )

        if response.usage:
            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
        
        raw_response_content = response.choices[0].message.content
        if raw_response_content:
            segments = [seg.strip() for seg in raw_response_content.split("||NEW_SEGMENT||") if seg.strip()]
            if not segments:
                 print(f"C·∫£nh b√°o: API tr·∫£ v·ªÅ n·ªôi dung nh∆∞ng kh√¥ng t√¨m th·∫•y ph√¢n ƒëo·∫°n h·ª£p l·ªá sau khi t√°ch b·∫±ng '||NEW_SEGMENT||'. N·ªôi dung th√¥: {raw_response_content[:200]}...")
        else:
            print("L·ªói: API tr·∫£ v·ªÅ n·ªôi dung tr·ªëng cho vi·ªác t·∫°o t·ª± s·ª± m·ªõi.")

    except Exception as e:
        print(f"L·ªói khi t·∫°o ph√¢n ƒëo·∫°n t·ª± s·ª± m·ªõi: {e}")
        # segments will remain empty

    if segments:
        print(f"ƒê√£ t·∫°o ƒë∆∞·ª£c {len(segments)} ph√¢n ƒëo·∫°n t·ª± s·ª± m·ªõi.")
    else:
        print("Kh√¥ng t·∫°o ƒë∆∞·ª£c ph√¢n ƒëo·∫°n t·ª± s·ª± n√†o.")
        
    return segments, prompt_tokens, completion_tokens

def rewrite_single_block_content(original_text: str, model: str, rewrite_instruction: str) -> tuple[str, int, int]:
    """
    Rewrites the text content of a single SRT block using OpenAI API.
    Returns the rewritten text, prompt tokens, and completion tokens.
    """
    if not original_text.strip():
        return original_text, 0, 0 # Return original if empty or only whitespace

    system_message_content = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω vi·∫øt l·∫°i n·ªôi dung chuy√™n nghi·ªáp. "
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt l·∫°i vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p theo h∆∞·ªõng d·∫´n c·ª• th·ªÉ, "
        "gi·ªØ nguy√™n √Ω nghƒ©a c·ªët l√µi nh∆∞ng thay ƒë·ªïi vƒÉn phong ho·∫∑c c·∫•u tr√∫c theo y√™u c·∫ßu."
    )
    
    user_prompt = (
        f"{rewrite_instruction}\n\n"
        "VƒÉn b·∫£n g·ªëc:\n"
        f"'''{original_text}'''\n\n"
        "Ch·ªâ tr·∫£ v·ªÅ ph·∫ßn vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i, kh√¥ng th√™m b·∫•t k·ª≥ l·ªùi gi·∫£i th√≠ch hay b√¨nh lu·∫≠n n√†o kh√°c."
    )

    prompt_tokens = 0
    completion_tokens = 0
    rewritten_text = f"[L·ªñI VI·∫æT L·∫†I - {original_text[:30]}...]" # Default error message

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_message_content},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7, # Adjust temperature as needed for creativity vs. fidelity
            max_tokens=1024  # Max tokens for the rewritten output of a single block
        )

        if response.usage:
            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
        
        rewritten_text_from_api = response.choices[0].message.content
        if rewritten_text_from_api:
            rewritten_text = rewritten_text_from_api.strip()
        else:
            print(f"L·ªói: API tr·∫£ v·ªÅ n·ªôi dung tr·ªëng cho kh·ªëi: {original_text[:50]}...")
            # Keep default error message for this case

    except Exception as e:
        print(f"L·ªói khi vi·∫øt l·∫°i kh·ªëi '{original_text[:50]}...': {e}")
        # Keeps the default error message with original text snippet

    return rewritten_text, prompt_tokens, completion_tokens

def rewrite_batch_content(original_texts_in_batch: list[tuple[str, str]], model: str, rewrite_instruction: str, context_summary: str) -> tuple[list[str], int, int]:
    """
    Rewrites a batch of SRT text blocks using OpenAI API with a single call, using context summary.
    original_texts_in_batch is a list of tuples (item_id, text_to_rewrite).
    Returns a list of rewritten texts in the same order, prompt tokens, and completion tokens.
    """
    if not original_texts_in_batch:
        return [], 0, 0

    prompt_text_parts = []
    for item_id, text_content in original_texts_in_batch:
        escaped_text_content = text_content.replace("'''", "\'\'\'")
        prompt_text_parts.append(f'{item_id}: """{escaped_text_content}"""')
    
    prompt_text_for_api = "\n".join(prompt_text_parts)

    system_message_content = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω vi·∫øt l·∫°i n·ªôi dung chuy√™n nghi·ªáp. "
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt l·∫°i c√°c ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p theo h∆∞·ªõng d·∫´n c·ª• th·ªÉ, "
        "gi·ªØ nguy√™n √Ω nghƒ©a c·ªët l√µi nh∆∞ng thay ƒë·ªïi vƒÉn phong ho·∫∑c c·∫•u tr√∫c theo y√™u c·∫ßu. "
        "S·ª≠ d·ª•ng t√≥m t·∫Øt k·ªãch b·∫£n ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n v√† ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh. "
        "X·ª≠ l√Ω t·ª´ng m·ª•c vƒÉn b·∫£n m·ªôt c√°ch ƒë·ªôc l·∫≠p."
    )

    user_prompt = (
        f"{rewrite_instruction}\n\n"
        "H√£y vi·∫øt l·∫°i c√°c ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c ƒë√°nh s·ªë ID d∆∞·ªõi ƒë√¢y. "
        "Tham kh·∫£o T√ìM T·∫ÆT K·ªäCH B·∫¢N sau ƒë·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ b·ªëi c·∫£nh:\n\n"
        f"T√ìM T·∫ÆT K·ªäCH B·∫¢N:\n'''{context_summary}'''\n\n"
        "Cung c·∫•p c√°c phi√™n b·∫£n ƒë√£ vi·∫øt l·∫°i trong m·ªôt ƒë·ªëi t∆∞·ª£ng JSON h·ª£p l·ªá, "
        "trong ƒë√≥ m·ªói kh√≥a l√† ID c·ªßa m·ª•c (v√≠ d·ª•: 'item_0') v√† gi√° tr·ªã l√† vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i t∆∞∆°ng ·ª©ng.\n\n"
        "VƒÉn b·∫£n g·ªëc c·∫ßn vi·∫øt l·∫°i:\n"
        f"{prompt_text_for_api}\n\n"
        "V√≠ d·ª• ƒë·ªãnh d·∫°ng JSON ƒë·∫ßu ra: {\"item_0\": \"vƒÉn b·∫£n vi·∫øt l·∫°i cho item_0\", \"item_1\": \"vƒÉn b·∫£n vi·∫øt l·∫°i cho item_1\", ...}"
    )

    prompt_tokens = 0
    completion_tokens = 0
    rewritten_texts_for_batch = [f"[L·ªñI VI·∫æT L·∫†I BATCH - {text[:30]}...]" for _, text in original_texts_in_batch] 

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_message_content},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=3500, 
            response_format={"type": "json_object"}
        )

        if response.usage:
            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
        
        rewritten_content_json_str = response.choices[0].message.content
        if not rewritten_content_json_str:
            print(f"L·ªói: API tr·∫£ v·ªÅ n·ªôi dung tr·ªëng cho batch.")
            return rewritten_texts_for_batch, prompt_tokens, completion_tokens

        rewritten_content_json = json.loads(rewritten_content_json_str)

        for i, (item_id, original_text) in enumerate(original_texts_in_batch):
            rewritten_text = rewritten_content_json.get(item_id)
            if rewritten_text:
                rewritten_texts_for_batch[i] = rewritten_text.strip()
            else:
                print(f"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y m·ª•c '{item_id}' trong ph·∫£n h·ªìi JSON cho batch. Gi·ªØ nguy√™n vƒÉn b·∫£n g·ªëc ho·∫∑c l·ªói.")
                rewritten_texts_for_batch[i] = f"[L·ªñI - KH√îNG T√åM TH·∫§Y {item_id} TRONG PH·∫¢N H·ªíI BATCH]"

    except json.JSONDecodeError as e:
        print(f"L·ªói khi gi·∫£i m√£ JSON t·ª´ API cho batch: {e}. D·ªØ li·ªáu nh·∫≠n ƒë∆∞·ª£c: {rewritten_content_json_str if 'rewritten_content_json_str' in locals() else 'Kh√¥ng c√≥ d·ªØ li·ªáu'}")
    except Exception as e:
        print(f"L·ªói khi vi·∫øt l·∫°i batch: {e}")
            
    return rewritten_texts_for_batch, prompt_tokens, completion_tokens

def calculate_cost(prompt_tokens: int, completion_tokens: int, model: str) -> float:
    """Calculates the estimated cost based on token usage and model pricing."""
    pricing = MODEL_PRICING.get(model)
    if not pricing:
        print(f"C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y th√¥ng tin gi√° cho model '{model}'. Chi ph√≠ s·∫Ω kh√¥ng ƒë∆∞·ª£c t√≠nh.")
        return 0.0

    prompt_cost = (prompt_tokens / 1000) * pricing["prompt"]
    completion_cost = (completion_tokens / 1000) * pricing["completion"]
    total_cost = prompt_cost + completion_cost
    return total_cost

def save_results_to_file(output_path: str, results: list[str]):
    """Saves the rewritten SRT blocks to the specified output file."""
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n\n".join(results)) 
    except Exception as e:
        print(f"L·ªói khi l∆∞u file {output_path}: {e}")

# --- Main Rewriting Function ---

def rewrite_srt_script_main(
    input_path: str, 
    output_path_placeholder: str, # Placeholder, actual output path determined by mode
    model: str, 
    rewrite_instruction_block: str, # Instruction for block-by-block
    batch_size: int, 
    summary_model: str, 
    title_model: str, # New parameter
    description_model: str, # New parameter
    summary_file_path: str,
    rewrite_mode: str,
    block_rewrite_style: str # "formal" or "creative_text"
):
    """Main function to rewrite/regenerate an SRT file's content based on the selected mode."""
    srt_blocks_full = load_srt_blocks(input_path)
    if not srt_blocks_full and rewrite_mode == "block_by_block_rewrite": # Only critical if blocks are needed
        print("Kh√¥ng c√≥ n·ªôi dung SRT ƒë·ªÉ x·ª≠ l√Ω cho ch·∫ø ƒë·ªô vi·∫øt l·∫°i theo kh·ªëi. K·∫øt th√∫c.")
        return
    if not srt_blocks_full and rewrite_mode == "narrative_regeneration" and not os.path.exists(summary_file_path):
        print("Kh√¥ng c√≥ n·ªôi dung SRT ƒë·ªÉ t·∫°o t√≥m t·∫Øt cho ch·∫ø ƒë·ªô t·∫°o t·ª± s·ª± m·ªõi v√† kh√¥ng c√≥ t√≥m t·∫Øt s·∫µn. K·∫øt th√∫c.")
        return

    grand_total_prompt_tokens = 0
    grand_total_completion_tokens = 0
    total_estimated_cost = 0.0

    # --- Step 1: Contextual Summary, Titles, Description (Common for both modes) ---
    print("\n--- B∆∞·ªõc 1: T·∫°o/T·∫£i t√≥m t·∫Øt, ti√™u ƒë·ªÅ & m√¥ t·∫£ (cho vi·∫øt l·∫°i) ---")
    context_summary, generated_titles, generated_description, s_t_d_p_tokens, s_t_d_c_tokens = get_or_create_summary(
        srt_blocks_full if srt_blocks_full else [], 
        summary_model, 
        title_model, 
        description_model, 
        summary_file_path
    )
    grand_total_prompt_tokens += s_t_d_p_tokens
    grand_total_completion_tokens += s_t_d_c_tokens

    # Combined cost for summary, titles, description
    s_t_d_cost = 0.0
    if s_t_d_p_tokens > 0 or s_t_d_c_tokens > 0:
        # Approximating cost with summary_model, or title_model if different pricing tier
        s_t_d_cost = calculate_cost(s_t_d_p_tokens, s_t_d_c_tokens, title_model) 
        print(f"Chi ph√≠ t√≥m t·∫Øt, ti√™u ƒë·ªÅ & m√¥ t·∫£ (∆∞·ªõc t√≠nh v·ªõi model: {title_model}): ${s_t_d_cost:.6f}")
    total_estimated_cost += s_t_d_cost

    if not context_summary:
        print("C·∫¢NH B√ÅO: Kh√¥ng th·ªÉ t·∫°o ho·∫∑c t·∫£i t√≥m t·∫Øt. Ch·ª©c nƒÉng c√≥ th·ªÉ b·ªã h·∫°n ch·∫ø.")
        if rewrite_mode == "narrative_regeneration":
            print("Kh√¥ng th·ªÉ ti·∫øp t·ª•c ch·∫ø ƒë·ªô t·∫°o t·ª± s·ª± m·ªõi m√† kh√¥ng c√≥ t√≥m t·∫Øt. K·∫øt th√∫c.")
            return
    else:
        print("\n--- Th√¥ng tin b·ªï tr·ª£ (vi·∫øt l·∫°i) ƒë√£ t·∫°o/t·∫£i ---")
        print(f"T√≥m t·∫Øt ng·ªØ c·∫£nh:\n{context_summary}")
        if generated_titles:
            print("\nC√°c ƒë·ªÅ xu·∫•t ti√™u ƒë·ªÅ (cho vi·∫øt l·∫°i):")
            for i, title in enumerate(generated_titles):
                print(f"  {i+1}. {title}")
        else:
            print("\nKh√¥ng c√≥ ti√™u ƒë·ªÅ n√†o ƒë∆∞·ª£c t·∫°o/t·∫£i (cho vi·∫øt l·∫°i).")
        if generated_description:
            print(f"\nM√¥ t·∫£ ƒë·ªÅ xu·∫•t (cho vi·∫øt l·∫°i):\n{generated_description}")
        else:
            print("\nKh√¥ng c√≥ m√¥ t·∫£ n√†o ƒë∆∞·ª£c t·∫°o/t·∫£i (cho vi·∫øt l·∫°i).")
    
    # Determine actual output path based on mode and original output_path_placeholder structure
    output_directory = os.path.dirname(output_path_placeholder)
    base_name_with_identifier = os.path.basename(output_path_placeholder).replace(".srt", "") #e.g. 0522_placeholder_timestamp
    original_base_name = base_name_with_identifier.split("_")[0] #e.g. 0522
    # The identifier is now a timestamp, not a UUID
    run_identifier = base_name_with_identifier.split("_")[-1] #e.g. timestamp_str 

    actual_output_path = ""

    # --- Mode Selection ---
    if rewrite_mode == "block_by_block_rewrite":
        print(f"\n--- Ch·∫ø ƒë·ªô: Vi·∫øt l·∫°i theo kh·ªëi (Block-by-Block Rewrite) - Ki·ªÉu: {block_rewrite_style} ---")
        if not srt_blocks_full:
            print("Kh√¥ng c√≥ kh·ªëi SRT n√†o ƒë·ªÉ vi·∫øt l·∫°i. K·∫øt th√∫c.")
            return
        
        actual_output_filename = f"{original_base_name}_block_rewrite_{block_rewrite_style}_{run_identifier}.srt"
        actual_output_path = os.path.join(output_directory, actual_output_filename)

        print(f"S·ª≠ d·ª•ng model vi·∫øt l·∫°i: {model}, k√≠ch th∆∞·ªõc batch: {batch_size}")
        print(f"H∆∞·ªõng d·∫´n vi·∫øt l·∫°i: {rewrite_instruction_block}")
        if context_summary:
            print("S·ª≠ d·ª•ng t√≥m t·∫Øt ng·ªØ c·∫£nh ƒë·ªÉ h·ªó tr·ª£ vi·∫øt l·∫°i.")
        else:
            print("C·∫¢NH B√ÅO: Vi·∫øt l·∫°i m√† kh√¥ng c√≥ t√≥m t·∫Øt ng·ªØ c·∫£nh b·ªï sung.")

        rewrite_section_p_tokens = 0
        rewrite_section_c_tokens = 0
        
        structured_srt_entries = []
        for block_idx, block_text_original in enumerate(srt_blocks_full):
            lines = block_text_original.split("\n")
            if len(lines) < 3:
                structured_srt_entries.append({'type': 'malformed', 'content': block_text_original})
            else:
                index, timecode = lines[0], lines[1]
                text_to_rewrite = "\n".join(lines[2:])
                structured_srt_entries.append({
                    'type': 'valid',
                    'id': f"item_{block_idx}",
                    'index': index,
                    'timecode': timecode,
                    'original_text': text_to_rewrite
                })

        valid_entries_to_process = [entry for entry in structured_srt_entries if entry['type'] == 'valid']
        batches = [valid_entries_to_process[i:i + batch_size] for i in range(0, len(valid_entries_to_process), batch_size)]
        processed_text_map = {}

        for batch_of_entries in tqdm(batches, desc="ƒêang vi·∫øt l·∫°i c√°c batch", unit="batch"):
            texts_for_api_batch = [(entry['id'], entry['original_text']) for entry in batch_of_entries]
            if not texts_for_api_batch: continue

            rewritten_texts_for_current_batch, p_tokens, c_tokens = rewrite_batch_content(
                texts_for_api_batch, model, rewrite_instruction_block, context_summary if context_summary else ""
            )
            
            rewrite_section_p_tokens += p_tokens
            rewrite_section_c_tokens += c_tokens

            for entry, rewritten_text in zip(batch_of_entries, rewritten_texts_for_current_batch):
                processed_text_map[entry['id']] = rewritten_text
            
            current_all_rewritten_srt_blocks = []
            for entry in structured_srt_entries:
                if entry['type'] == 'malformed':
                    current_all_rewritten_srt_blocks.append(entry['content'])
                elif entry['type'] == 'valid':
                    rewritten_content = processed_text_map.get(entry['id'], entry['original_text']) 
                    current_all_rewritten_srt_blocks.append(f"{entry['index']}\n{entry['timecode']}\n{rewritten_content}")
            save_results_to_file(actual_output_path, current_all_rewritten_srt_blocks)
            time.sleep(1.0)
        
        grand_total_prompt_tokens += rewrite_section_p_tokens
        grand_total_completion_tokens += rewrite_section_c_tokens

        rewrite_cost = calculate_cost(rewrite_section_p_tokens, rewrite_section_c_tokens, model)
        print(f"Chi ph√≠ vi·∫øt l·∫°i (model: {model}): ${rewrite_cost:.6f}")
        total_estimated_cost += rewrite_cost
        print(f"K·ªãch b·∫£n vi·∫øt l·∫°i theo kh·ªëi ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {actual_output_path}")

    elif rewrite_mode == "narrative_regeneration":
        print(f"\n--- Ch·∫ø ƒë·ªô: T·∫°o t·ª± s·ª± m·ªõi (Narrative Re-generation) ---")
        if not context_summary:
            print("Kh√¥ng c√≥ t√≥m t·∫Øt, kh√¥ng th·ªÉ t·∫°o t·ª± s·ª± m·ªõi. K·∫øt th√∫c.")
            return # Already checked, but as a safeguard

        actual_output_filename = f"{original_base_name}_narrative_regen_{run_identifier}.srt"
        actual_output_path = os.path.join(output_directory, actual_output_filename)
        
        print(f"S·ª≠ d·ª•ng model t·∫°o t·ª± s·ª±: {model}")
        narrative_goal_prompt_template = (
            "B·∫°n L√Ä B√† Di·ªáu An, m·ªôt ng∆∞·ªùi k·ªÉ chuy·ªán l·ªõn tu·ªïi, ƒë·ª©c ƒë·ªô v√† am hi·ªÉu s·ª± ƒë·ªùi. H√£y ho√†n to√†n nh·∫≠p vai B√† Di·ªáu An. "
            "QUAN TR·ªåNG V·ªÄ X∆ØNG H√î: Khi k·ªÉ chuy·ªán, h√£y LU√îN LU√îN x∆∞ng l√† 'b√†' (v√≠ d·ª•: 'b√† k·ªÉ cho c√°c con nghe', 'ng√†y ƒë√≥ b√† c√≤n tr·∫ª...') v√† g·ªçi ng∆∞·ªùi nghe (kh√°n gi·∫£) l√† 'c√°c con' (v√≠ d·ª•: 'c√°c con c√≥ bi·∫øt kh√¥ng?', 'chuy·ªán l√† th·∫ø n√†y n√® c√°c con...'). "
            "H√£y k·ªÉ l·∫°i c√¢u chuy·ªán n√†y nh∆∞ th·ªÉ ch√≠nh b√† ƒëang th·ªß th·ªâ, t√¢m t√¨nh, chia s·∫ª nh·ªØng k√Ω ·ª©c s√¢u s·∫Øc c·ªßa m√¨nh tr·ª±c ti·∫øp v·ªõi c√°c con. L·ªùi k·ªÉ c·ªßa b√† ph·∫£i th·∫≠t t·ª± nhi√™n, ch√¢n th√†nh, th·ªÉ hi·ªán ƒë√∫ng vai v·∫ø v√† t√¨nh c·∫£m c·ªßa m·ªôt ng∆∞·ªùi b√† v·ªõi c√°c con ch√°u. L·ªùi k·ªÉ c·ªßa b√† ch√≠nh l√† l·ªùi c·ªßa B√† Di·ªáu An, kh√¥ng ph·∫£i l√† m·ªôt s·ª± t∆∞·ªùng thu·∫≠t l·∫°i t·ª´ m·ªôt ng∆∞·ªùi th·ª© ba hay m·ªôt di·ªÖn vi√™n ƒë√≥ng vai. "
            
            "L∆ØU √ù ƒê·∫∂C BI·ªÜT V·ªÄ NH√ÇN V·∫¨T TRONG K·ªäCH B·∫¢N G·ªêC: Trong k·ªãch b·∫£n g·ªëc, n·∫øu c√≥ nh·∫Øc ƒë·∫øn t√™n 'Di·ªáu Huy·ªÅn', c√°c con h√£y hi·ªÉu r·∫±ng ƒë√≥ ch√≠nh l√† b√† (B√† Di·ªáu An) khi c√≤n tr·∫ª ho·∫∑c trong m·ªôt b·ªëi c·∫£nh kh√°c c·ªßa c√¢u chuy·ªán. Khi k·ªÉ l·∫°i, b√† s·∫Ω lu√¥n x∆∞ng l√† 'b√†' khi n√≥i v·ªÅ m√¨nh trong vai Di·ªáu Huy·ªÅn/B√† Di·ªáu An, v√† k·ªÉ t·ª´ g√≥c nh√¨n c·ªßa b√†, nh∆∞ ƒëang k·ªÉ cho c√°c con nghe v·ªÅ qu√° kh·ª© c·ªßa m√¨nh. "
            
            "K·ªãch b·∫£n t·ª± s·ª± m·ªõi n√†y ph·∫£i b√°m s√°t ch·∫∑t ch·∫Ω c√°c s·ª± ki·ªán, th√¥ng tin v√† tr√¨nh t·ª± c·ªßa K·ªäCH B·∫¢N G·ªêC ƒê·∫¶Y ƒê·ª¶. "
            "S·ª≠ d·ª•ng T√ìM T·∫ÆT K·ªäCH B·∫¢N ƒë·ªÉ n·∫Øm b·∫Øt gi·ªçng ƒëi·ªáu v√† b·ªëi c·∫£nh chung, nh∆∞ng to√†n b·ªô l·ªùi k·ªÉ v√† c√°ch x∆∞ng h√¥ ph·∫£i l√† c·ªßa B√† Di·ªáu An n√≥i v·ªõi c√°c con. "
            "{duration_guidance} " 
            "Chia k·ªãch b·∫£n th√†nh nhi·ªÅu ƒëo·∫°n vƒÉn ng·∫Øn (kho·∫£ng 2-4 c√¢u m·ªói ƒëo·∫°n), m·ªói ƒëo·∫°n vƒÉn s·∫Ω t∆∞∆°ng ·ª©ng v·ªõi m·ªôt kh·ªëi ph·ª• ƒë·ªÅ. L·ªùi vƒÉn c·∫ßn t·ª± nhi√™n, mang ƒë·∫≠m t√≠nh k·ªÉ chuy·ªán c·ªßa B√† Di·ªáu An (x∆∞ng 'b√†', g·ªçi 'c√°c con'), c√≥ th·ªÉ c√≥ nh·ªØng l·ªùi b√¨nh lu·∫≠n √Ω nh·ªã ho·∫∑c chi√™m nghi·ªám c·ªßa ch√≠nh b√†. Khi m√¥ t·∫£ c√°c nh√¢n v·∫≠t kh√°c ho·∫∑c ƒë∆∞a ra nh·ªØng l·ªùi b√¨nh/chi√™m nghi·ªám n√†y, h√£y ƒë·∫£m b·∫£o gi·ªçng ƒëi·ªáu, c√°ch nh√¨n, v√† ng√¥n t·ª´ ph·∫£i ho√†n to√†n l√† c·ªßa B√† Di·ªáu An ‚Äì m·ªôt ng∆∞·ªùi ph·ª• n·ªØ l·ªõn tu·ªïi, t·ª´ng tr·∫£i, am hi·ªÉu s·ª± ƒë·ªùi, ƒëang k·ªÉ chuy·ªán cho c√°c con nghe. To√†n b·ªô l·ªùi k·ªÉ, bao g·ªìm c·∫£ vi·ªác m√¥ t·∫£ nh√¢n v·∫≠t v√† s·ª± ki·ªán, ƒë·ªÅu ph·∫£i nh·∫•t qu√°n v·ªõi vai k·ªÉ n√†y. "
            "T·∫≠p trung v√†o vi·ªác k·ªÉ m·ªôt c√¢u chuy·ªán m·∫°ch l·∫°c, thu h√∫t s·ª± ch√∫ √Ω c·ªßa c√°c con, c√≥ th·ªÉ bao g·ªìm c√°c y·∫øu t·ªë b·∫•t ng·ªù ho·∫∑c c·∫£m x√∫c, nh∆∞ng kh√¥ng ƒë∆∞·ª£c thay ƒë·ªïi c√°c s·ª± th·∫≠t c·ªët l√µi t·ª´ k·ªãch b·∫£n g·ªëc (ngo·∫°i tr·ª´ vi·ªác th·ªëng nh·∫•t t√™n g·ªçi v√† c√°ch x∆∞ng h√¥ nh∆∞ ƒë√£ n√™u tr√™n). "
            "M·ª•c ti√™u l√† t·∫°o ra m·ªôt tr·∫£i nghi·ªám xem m·ªõi m·∫ª, v·ªõi gi·ªçng k·ªÉ ƒë·∫∑c tr∆∞ng, th√¢n m·∫≠t v√† tr·ª±c ti·∫øp c·ªßa B√† Di·ªáu An (b√† x∆∞ng 'b√†', g·ªçi 'c√°c con'), c√≥ th·ªÉ d√πng cho h√¨nh ·∫£nh g·ªëc ho·∫∑c g·ª£i √Ω h√¨nh ·∫£nh m·ªõi, ƒë·ªìng th·ªùi v·∫´n truy·ªÅn t·∫£i trung th·ª±c n·ªôi dung g·ªëc. "
            "Vui l√≤ng tr·∫£ v·ªÅ m·ªôt danh s√°ch c√°c ƒëo·∫°n vƒÉn b·∫£n (ph√¢n t√°ch b·∫±ng m·ªôt d·∫•u xu·ªëng d√≤ng ƒë·∫∑c bi·ªát nh∆∞ '||NEW_SEGMENT||' gi·ªØa c√°c ƒëo·∫°n) m√† kh√¥ng c√≥ b·∫•t k·ª≥ ƒë·ªãnh d·∫°ng JSON hay ƒë√°nh s·ªë n√†o."
        )
        
        original_srt_full_text = "\n\n".join(srt_blocks_full) 
        original_duration_seconds = get_srt_total_duration(srt_blocks_full)
        print(f"T·ªïng th·ªùi l∆∞·ª£ng k·ªãch b·∫£n g·ªëc ∆∞·ªõc t√≠nh: {original_duration_seconds:.2f} gi√¢y.")

        print("ƒêang t·∫°o t·ª± s·ª± m·ªõi...")
        segments, nr_p_tokens, nr_c_tokens = generate_new_narrative_segments(
            context_summary if context_summary else "", 
            original_srt_full_text,
            original_duration_seconds,
            model, 
            narrative_goal_prompt_template
        )

        if segments:
            generated_srt_content = []
            start_time_ms = 0
            
            # Distribute original duration among segments
            num_segments = len(segments)
            if num_segments > 0 and original_duration_seconds > 0:
                avg_duration_per_segment_ms = (original_duration_seconds * 1000) / num_segments
            else:
                avg_duration_per_segment_ms = NARRATIVE_SEGMENT_DURATION_SECONDS * 1000 # Fallback
            
            # Ensure minimum duration to avoid overly short segments if original is very short or many segments generated
            min_segment_duration_ms = 3000 # 3 seconds minimum
            segment_duration_ms = max(avg_duration_per_segment_ms, min_segment_duration_ms)

            print(f"S·∫Ω t·∫°o {num_segments} kh·ªëi ph·ª• ƒë·ªÅ m·ªõi, m·ªói kh·ªëi kho·∫£ng {segment_duration_ms/1000:.2f} gi√¢y.")
            for i, seg_text in enumerate(tqdm(segments, desc="ƒêang t·∫°o c√°c kh·ªëi SRT t·ª´ t·ª± s·ª± m·ªõi", unit="kh·ªëi")):
                end_time_ms = start_time_ms + segment_duration_ms
                
                # Ensure ms components are integers for formatting
                current_start_ms_int = int(start_time_ms % 1000)
                current_end_ms_int = int(end_time_ms % 1000)

                start_hms = time.strftime('%H:%M:%S', time.gmtime(start_time_ms // 1000))
                end_hms = time.strftime('%H:%M:%S', time.gmtime(end_time_ms // 1000))
                
                timecode = f"{start_hms},{current_start_ms_int:03d} --> {end_hms},{current_end_ms_int:03d}"
                generated_srt_content.append(f"{i+1}\n{timecode}\n{seg_text}")
                start_time_ms = end_time_ms + 200 # Small gap between subtitles
            save_results_to_file(actual_output_path, generated_srt_content)
            print(f"K·ªãch b·∫£n t·ª± s·ª± m·ªõi (v·ªõi th·ªùi gian t·ª± ƒë·ªông c∆° b·∫£n) ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {actual_output_path}")

            grand_total_prompt_tokens += nr_p_tokens
            grand_total_completion_tokens += nr_c_tokens
            narrative_regen_cost = calculate_cost(nr_p_tokens, nr_c_tokens, model)
            print(f"Chi ph√≠ t·∫°o t·ª± s·ª± m·ªõi (model: {model}): ${narrative_regen_cost:.6f}")
            total_estimated_cost += narrative_regen_cost
        else:
            print("Kh√¥ng t·∫°o ƒë∆∞·ª£c ph√¢n ƒëo·∫°n t·ª± s·ª± m·ªõi.")

    else:
        print(f"L·ªñI: Ch·∫ø ƒë·ªô vi·∫øt l·∫°i '{rewrite_mode}' kh√¥ng h·ª£p l·ªá.")
        return

    print("\n--- Ho√†n Th√†nh X·ª≠ L√Ω ---")
    # print(f"K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i c√°c ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ·ª©ng v·ªõi ch·∫ø ƒë·ªô ƒë√£ ch·ªçn.") # More specific messages are printed within modes
    print(f"T·ªïng s·ªë prompt tokens (bao g·ªìm t√≥m t·∫Øt, v√† vi·∫øt l·∫°i/t·∫°o t·ª± s·ª±): {grand_total_prompt_tokens}")
    print(f"T·ªïng s·ªë completion tokens (bao g·ªìm t√≥m t·∫Øt, v√† vi·∫øt l·∫°i/t·∫°o t·ª± s·ª±): {grand_total_completion_tokens}")
    print(f"T·ªïng ∆∞·ªõc t√≠nh chi ph√≠ cho phi√™n l√†m vi·ªác: ${total_estimated_cost:.6f}")


# --- Script Execution ---

if __name__ == "__main__":
    INPUT_FILE_PATH = "data/0523.srt"  
    REWRITE_MODEL_NAME = "gpt-4o-mini" # Used for both block rewrite and narrative regen for now
    SUMMARIZATION_MODEL_NAME = DEFAULT_SUMMARIZATION_MODEL
    TITLE_DESCRIPTION_MODEL_NAME = DEFAULT_TITLE_DESCRIPTION_MODEL # New constant for rewrite context
    SRT_BATCH_SIZE = DEFAULT_BATCH_SIZE # Only for block_by_block_rewrite mode
    
    # --- CHOOSE OPERATIONAL MODE ---
    REWRITE_MODE = "block_by_block_rewrite"  # Options: "narrative_regeneration", "block_by_block_rewrite"
    # -------------------------------

    # --- CHOOSE REWRITE STYLE (Only for REWRITE_MODE = "block_by_block_rewrite") ---
    BLOCK_REWRITE_STYLE = "creative_text" # Options: "formal", "creative_text"
    # ----------------------------------------------------------------------------

    REWRITE_INSTRUCTION_FORMAL = (
        "H√£y vi·∫øt l·∫°i ƒëo·∫°n vƒÉn b·∫£n sau ƒë√¢y v·ªõi vƒÉn phong trang tr·ªçng h∆°n, "
        "th√≠ch h·ª£p cho m·ªôt gi·ªçng ƒë·ªçc phim t√†i li·ªáu. Gi·ªØ nguy√™n √Ω nghƒ©a c·ªët l√µi c·ªßa vƒÉn b·∫£n. "
        "Kh√¥ng thay ƒë·ªïi t√™n ri√™ng ho·∫∑c c√°c thu·∫≠t ng·ªØ k·ªπ thu·∫≠t tr·ª´ khi ƒë∆∞·ª£c y√™u c·∫ßu."
    )

    REWRITE_INSTRUCTION_CREATIVE_TEXT = (
        "H√£y vi·∫øt l·∫°i ƒëo·∫°n vƒÉn b·∫£n sau ƒë√¢y ƒë·ªÉ tr·ªü n√™n h·∫•p d·∫´n, thu h√∫t s·ª± t√≤ m√≤ c·ªßa kh√°n gi·∫£ ƒë·∫°i ch√∫ng. "
        "B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t k·ªÉ chuy·ªán, th√™m y·∫øu t·ªë g√¢y c·∫•n, ho·∫∑c ƒë∆°n gi·∫£n h√≥a c√°c √Ω t∆∞·ªüng ph·ª©c t·∫°p ƒë·ªÉ n·ªôi dung d·ªÖ ti·∫øp c·∫≠n v√† d·ªÖ lan t·ªèa h∆°n. "
        "M·ª•c ti√™u l√† t·ªëi ƒëa h√≥a s·ª± duy tr√¨ v√† h·ª©ng th√∫ c·ªßa ng∆∞·ªùi xem, ngay c·∫£ khi ƒëi·ªÅu ƒë√≥ c√≥ nghƒ©a l√† thay ƒë·ªïi m·ªôt ch√∫t c√°ch di·ªÖn ƒë·∫°t tr·ª±c ti·∫øp c·ªßa k·ªãch b·∫£n g·ªëc, nh∆∞ng v·∫´n ph·∫£i gi·ªØ ƒë∆∞·ª£c c√°c s·ª± ki·ªán ho·∫∑c th√¥ng tin c·ªët l√µi. "
        "Tham kh·∫£o t√≥m t·∫Øt ng·ªØ c·∫£nh ƒë·ªÉ ƒë·∫£m b·∫£o s·ª± s√°ng t·∫°o ph√π h·ª£p v·ªõi b·ª©c tranh to√†n c·∫£nh c·ªßa c√¢u chuy·ªán."
    )

    SELECTED_BLOCK_REWRITE_INSTRUCTION = ""
    output_filename_suffix_placeholder = "placeholder" # This will be refined by the main function

    if BLOCK_REWRITE_STYLE == "formal":
        SELECTED_BLOCK_REWRITE_INSTRUCTION = REWRITE_INSTRUCTION_FORMAL
    elif BLOCK_REWRITE_STYLE == "creative_text":
        SELECTED_BLOCK_REWRITE_INSTRUCTION = REWRITE_INSTRUCTION_CREATIVE_TEXT
    # No exit here if style is wrong, main function can use default or handle if mode is block_by_block

    # Generate a timestamp string for filenames
    timestamp_str = time.strftime("%Y%m%d_%H%M%S")

    if not os.path.exists(rewrite_scripts_DIRECTORY):
        os.makedirs(rewrite_scripts_DIRECTORY)
    if not os.path.exists(REWRITE_SUMMARIES_DIRECTORY):
        os.makedirs(REWRITE_SUMMARIES_DIRECTORY)

    base_name, _ = os.path.splitext(os.path.basename(INPUT_FILE_PATH))
    
    summary_file_name = f"{base_name}_rewrite_summary.txt"
    summary_file_path = os.path.join(REWRITE_SUMMARIES_DIRECTORY, summary_file_name)

    # This output_file_path is now more of a placeholder pattern for the main function to use
    # The actual name will be determined inside rewrite_srt_script_main based on the mode.
    output_file_name_placeholder = f"{base_name}_{output_filename_suffix_placeholder}_{timestamp_str}.srt" 
    output_file_path_placeholder = os.path.join(rewrite_scripts_DIRECTORY, output_file_name_placeholder)

    rewrite_srt_script_main(
        input_path=INPUT_FILE_PATH,
        output_path_placeholder=output_file_path_placeholder,
        model=REWRITE_MODEL_NAME,
        rewrite_instruction_block=SELECTED_BLOCK_REWRITE_INSTRUCTION, 
        batch_size=SRT_BATCH_SIZE,
        summary_model=SUMMARIZATION_MODEL_NAME,
        title_model=TITLE_DESCRIPTION_MODEL_NAME, # Pass title model
        description_model=TITLE_DESCRIPTION_MODEL_NAME, # Pass description model
        summary_file_path=summary_file_path,
        rewrite_mode=REWRITE_MODE,
        block_rewrite_style=BLOCK_REWRITE_STYLE
    ) 